{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc536687-d4a9-43f8-a2cf-61b8b80c63fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install vllm\n",
    "!pip install pyngrok --quiet\n",
    "! apt-get install coreutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14949c85-9cfe-440c-9024-5b7dda2b70bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n"
     ]
    }
   ],
   "source": [
    "! pip install -U hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e69fca-590b-4843-b7a0-d469dbd6214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "coreutils is already the newest version (9.4-3ubuntu6.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! apt-get install coreutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c68810-312f-4c76-b377-78042e54532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<NgrokTunnel: \"https://e6913f05c000.ngrok-free.app\" -> \"http://localhost:8000\">"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t=2025-11-07T11:59:32+0000 lvl=warn msg=\"failed to open private leg\" id=7a1e78b97527 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n",
      "t=2025-11-07T11:59:33+0000 lvl=warn msg=\"failed to open private leg\" id=6d758c502259 privaddr=localhost:8000 err=\"dial tcp 127.0.0.1:8000: connect: connection refused\"\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "ngrok.set_auth_token(\"2zg29dbFhuPIWAULFv3qIRUuMVE_4bZmyRWwxKvHkV9KaafLp\")\n",
    " \n",
    "public_url = ngrok.connect(8000)\n",
    "public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2bb8e-57a2-468c-91bf-8e7b43459d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-07 11:57:54 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 11:57:59 [api_server.py:1839] vLLM API server version 0.11.0\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 11:57:59 [utils.py:233] non-default args: {'model_tag': 'mistral-community/Pixtral-12B', 'api_key': ['your_key_if_needed'], 'model': 'mistral-community/Pixtral-12B', 'max_model_len': 32768}\n",
      "config.json: 100%|█████████████████████████████| 997/997 [00:00<00:00, 8.26MB/s]\n",
      "preprocessor_config.json: 100%|████████████████| 483/483 [00:00<00:00, 2.83MB/s]\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 11:58:08 [model.py:547] Resolved architecture: LlavaForConditionalGeneration\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 11:58:08 [model.py:1510] Using max model len 32768\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 11:58:10 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "tokenizer_config.json: 177kB [00:00, 91.3MB/s]\n",
      "tokenizer.json: 9.26MB [00:00, 211MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 3.47MB/s]\n",
      "generation_config.json: 100%|███████████████████| 116/116 [00:00<00:00, 637kB/s]\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m   warnings.warn(msg, FutureWarning)\n",
      "INFO 11-07 11:58:17 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:21 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:21 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='mistral-community/Pixtral-12B', speculative_config=None, tokenizer='mistral-community/Pixtral-12B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=mistral-community/Pixtral-12B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m   warnings.warn(msg, FutureWarning)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:23 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m WARNING 11-07 11:58:23 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "processor_config.json: 100%|███████████████████| 162/162 [00:00<00:00, 1.41MB/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "chat_template.json: 1.38kB [00:00, 3.97MB/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:27 [gpu_model_runner.py:2602] Starting to load model mistral-community/Pixtral-12B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:28 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:28 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 11:58:28 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "model-00001-of-00006.safetensors: 100%|█████| 4.99G/4.99G [00:39<00:00, 127MB/s]\n",
      "model-00002-of-00006.safetensors: 100%|█████| 4.96G/4.96G [00:38<00:00, 127MB/s]\n",
      "model-00003-of-00006.safetensors: 100%|█████| 4.91G/4.91G [00:41<00:00, 119MB/s]\n",
      "model-00004-of-00006.safetensors: 100%|█████| 4.91G/4.91G [00:38<00:00, 128MB/s]\n",
      "model-00005-of-00006.safetensors: 100%|█████| 4.26G/4.26G [00:34<00:00, 124MB/s]\n",
      "model-00006-of-00006.safetensors: 100%|█████| 1.34G/1.34G [00:11<00:00, 115MB/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:01:53 [weight_utils.py:413] Time spent downloading weights for mistral-community/Pixtral-12B: 204.822289 seconds\n",
      "model.safetensors.index.json: 57.9kB [00:00, 65.3MB/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:01<00:05,  1.03s/it]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:02<00:04,  1.20s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [00:03<00:03,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [00:04<00:02,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [00:05<00:01,  1.16s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:06<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [00:06<00:00,  1.06s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:01:59 [default_loader.py:267] Loading weights took 6.41 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:00 [gpu_model_runner.py:2653] Model loading took 23.8692 GiB and 211.783254 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:00 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 4160 tokens, and profiled with 1 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:07 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/e1dda608fa/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:07 [backends.py:559] Dynamo bytecode transform time: 6.65 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:11 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:34 [backends.py:218] Compiling a graph for dynamic shape takes 25.16 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:39 [monitor.py:34] torch.compile takes 31.81 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:40 [gpu_worker.py:298] Available KV cache memory: 17.56 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:41 [kv_cache_utils.py:1087] GPU KV cache size: 115,088 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:41 [kv_cache_utils.py:1091] Maximum concurrency for 32,768 tokens per request: 3.51x\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|█| 67/67 [00:07<00\n",
      "Capturing CUDA graphs (decode, FULL): 100%|█████| 35/35 [00:02<00:00, 12.61it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:52 [gpu_model_runner.py:3480] Graph capturing finished in 11 secs, took 0.85 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1057)\u001b[0;0m INFO 11-07 12:02:52 [core.py:210] init engine (profile, create kv cache, warmup model) took 51.86 seconds\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:53 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 7193\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:53 [api_server.py:1634] Supported_tasks: ['generate']\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:34] Available routes are:\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /docs, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /redoc, Methods: HEAD, GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /health, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /load, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /ping, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /ping, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /tokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /detokenize, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/models, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /version, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/responses, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/chat/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/completions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/embeddings, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /pooling, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /classify, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/score, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/audio/translations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v1/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /v2/rerank, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /invocations, Methods: POST\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m INFO 11-07 12:02:54 [launcher.py:42] Route: /metrics, Methods: GET\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m784\u001b[0m]\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     110.227.192.47:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     110.227.192.47:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
      "\u001b[1;36m(APIServer pid=784)\u001b[0;0m \u001b[32mINFO\u001b[0m:     110.227.192.47:0 - \"\u001b[1mPOST /v1/chat/completions HTTP/1.1\u001b[0m\" \u001b[31m401 Unauthorized\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pkill -f vllm.entrypoints.openai.api_server || true\n",
    "\n",
    "! vllm serve mistral-community/Pixtral-12B \\\n",
    "  --dtype auto \\\n",
    "  --max-model-len 32768 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9cea1-1b9a-4f82-acc1-7842bbf6a30b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3159af-f2b6-4755-adfc-fbaa747895e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
